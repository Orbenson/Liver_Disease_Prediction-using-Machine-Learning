# -*- coding: utf-8 -*-
"""Big_Data_Project_Liver_Disease_Patient_Dataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ityuw-W3-2T_L-Yzpbwt3ChZYeNF5mYn

### **Liver Disease Patient Dataset**

**Data set:**

The data Set contains 20K train data, ~1K Test data.

This data set contains 10 variables that are age, gender, total Bilirubin, direct Bilirubin, total proteins, albumin, A/G ratio, SGPT, SGOT and Alkphos.

From the website of Mount Sinai hospital we collected the data of the normal results of each feature.

https://www.mountsinai.org/health-library

**Attribute Information:**

1. Age - the age of the patient
2. Gender -  the gender of the patient
3. TB  - Total Bilirubin. This is a blood test that measures the amount of a substance called bilirubin. This test is used to find out how well your liver is working. It is often part of a panel of tests that measure liver function. A small amount of bilirubin in your blood is normal, but a high level may be a sign of liver disease. <br>
The normal range is between 0.1 to 1.2 mg/dL 
4. DB - Direct Bilirubin. <br>
The normal range is less than 0.3 mg/dL
5. Alkphos - Alkaline Phosphotas. <br>
The normal range is between 44 to 147 international units per literת IU/L
6. Sgpt - Alanine Aminotransferase, An enzyme found in the liver and other tissues. <br>
The normal range is between 4 to 36 U/L.
7. Sgot - Aspartate Aminotransferase also an enzyme found in the liver.  <br>
The normal range is between 8 to 33 U/L.
8. TP - Total Proteins. <br>
The normal range between is 6.0 to 8.3 grams per deciliterת g/dL
9. ALB - Albumin. <br>
The normal range is between 3.4 to 5.4 g/dL 
10. A/G  - The Ratio Albumin and Globulin Ratio. It measures the ratio of albumin to globulin, the two main proteins in your blood. The normal range for albumin/globulin ratio is over 1 Source, usually around 1 to 2. That’s because there’s a bit more albumin than globulin in serum protein.

Selector field used to split the data into two sets (labeled by the experts) 1 Liver Patient, 2 non Liver Patient we transformed the 2 into zero for convenience mesures later on)

### Install relevant packages
"""

!pip install pyspark

"""### Imports"""

# Commented out IPython magic to ensure Python compatibility.
# Import other modules not related to PySpark
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from sklearn.metrics import accuracy_score
import warnings
sns.set_style('darkgrid')
# %matplotlib inline

# Import PySpark related modules
import pyspark as sp
from pyspark.rdd import RDD
from pyspark.sql import Row
from pyspark.sql import DataFrame
from pyspark.sql import SparkSession
import pyspark.sql.functions as F
from pyspark.sql import SQLContext
from pyspark.sql import functions

import pyspark.sql.functions as func
from pyspark.sql.functions import lit,mean, desc, col, size, array_contains\
, isnan, udf, hour, array_min, array_max, countDistinct, when, sum,avg,max,count
from pyspark.ml.feature import Bucketizer
from pyspark.sql.types import *
from pyspark.sql.functions import col, count, mean
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes
from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator

warnings.filterwarnings('ignore')

"""### Set up spark environment"""

# Initialize a spark session
sc = sp.SparkContext.getOrCreate()
print(sc)
print(sc.version)

"""### Start Spark Session"""

# create a SparkSession
spark = SparkSession.builder.appName("Liver Patient Prediction").getOrCreate()

"""## Read datasets"""

# load the CSV file as a DataFrame
file_path = "Liver Patient Dataset (LPD)_train - Liver Patient Dataset (LPD)_train.csv"
df = spark.read.format("csv").option("header", True).option("inferSchema", True).load(file_path)

"""## **Overview of Dataset**

### Data Pre-Processing
"""

type(df)

# Check the data types of the columns in the DataFrame
df.printSchema()

pd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])

"""### Set coulmns names"""

# Change the coulms names 
Columns = ["age","gender","TB","DB","AAP","SgptAA","SgotAA","TP","ALBA","A/G ratio","target"]
df = df.toDF(*Columns)
df.show()

"""## **Detect missing values and abnormal zeroes**

### Start by checking the total number of rows we have
"""

# Check the number of rows and columns in the DataFrame
print("Number of rows: ", df.count())
print("Number of columns: ", len(df.columns))

df.groupby('target').count().show()

"""Change the target numbers to be 0/1 instead of 2/1 where:

1 - Diagnosed with liver disease 

0 - Is not diagnosed with liver disease 
"""

df = df.withColumn('target', when(df.target == 2,0).otherwise(df.target))
df.groupby('target').count().show()

"""After having a first sight of the columns, the first thing we should check is if the data set having any missing value.

*   For string columns, we check for None and null
*   For numeric columns, we check for zeroes and NaN
"""

print('Columns overview')
pd.DataFrame(df.dtypes, columns = ['Column Name','Data type'])

"""### Handle missing data"""

# Check the number of missing values in each column of the DataFrame
string_columns = ['gender']
numeric_columns = ['age','gender','TB','DB','AAP','SgptAA','SgotAA','TP','ALBA','A/G ratio','target']
missing_values = {} 

for index, column in enumerate(df.columns):
    if column in string_columns:    # check string columns with None and Null values
        missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()
        missing_values.update({column:missing_count})
    if column in numeric_columns:  # check zeroes, None, NaN
        missing_count = df.where(col(column).isNull()).count()
        missing_values.update({column:missing_count})
missing_df = pd.DataFrame.from_dict([missing_values])
missing_df

"""We will handle missing data as follows:
* Because gender is a categorical feature we will remove the missing data (it is less then 3% of the data) Thw same goes to age where there are 2 rows with missing data so we will remove them as well.
* For the nomerical features we will two options to. remove the data and take the risk of losing data or replace the null values with other value. We choose to replace the null values with the mean of each feature and with that we will not lose any data.

We will start with gender and age, first we will remove all null values and then we will convert it into inderxer:
"""

# Remove null values from gender and age
df = df.filter(df.gender.isNotNull())
df = df.filter(df.age.isNotNull())

print("Number of current rows: ", df.count())

# Convert the "gender" column to a numeric type using StringIndexer
indexer = StringIndexer(inputCol="gender", outputCol="genderIndex")
df = indexer.fit(df).transform(df)
df = df.withColumn("genderIndex", func.round(df["genderIndex"], 2).cast('integer'))

# Check the data types of the columns in the DataFrame again
df.printSchema()

df.show(5)

# Drop the original "gender" column and keep only the "genderIndex" column
df = df.drop("gender").withColumnRenamed("genderIndex", "gender")
df.show(5)

print('Data frame describe (string and numeric columns only):')
df.describe().toPandas()

"""For the nomerical features we will replace the null values with the mean of the feature:"""

# df_avg is a list of all the features mean or average values
df_avg = df.select(mean('age'), mean('TB'), mean('DB'), mean('AAP'), mean('SgptAA'), mean('SgotAA'), mean('TP'), mean('ALBA'), mean('A/G ratio')).collect()

#filling numeric column values with the mean or average value of that particular column
df = df.fillna({'age':df_avg[0][0],'TB':df_avg[0][1],'DB':df_avg[0][2],'AAP':df_avg[0][3],'SgptAA':df_avg[0][4],'SgotAA':df_avg[0][5],'TP':df_avg[0][6],'ALBA':df_avg[0][7],'A/G ratio':df_avg[0][8]})
df.show(5)

# Checking once again that we are not left with any null value
string_columns = ['gender']
numeric_columns = ['age','gender','TB','DB','AAP','SgptAA','SgotAA','TP','ALBA','A/G ratio','target']
missing_values = {} 

for index, column in enumerate(df.columns):
    if column in string_columns:    # check string columns with None and Null values
        missing_count = df.filter(col(column).eqNullSafe(None) | col(column).isNull()).count()
        missing_values.update({column:missing_count})
    if column in numeric_columns:  # check zeroes, None, NaN
        missing_count = df.where(col(column).isNull()).count()
        missing_values.update({column:missing_count})
missing_df = pd.DataFrame.from_dict([missing_values])
missing_df

"""### **Exploratory Data Analysis - EDA**

### One-dimensional analysis

To perform a one-dimensional analysis, value ranges must be grouped for each parameter.

Performing regressions and analyzes based on categorical variables has several notable advantages:


*   Ability to detect non-linear relationships (such as age: it is possible that borrowers aged 35-45 are the most dangerous).
*   Effect of extreme cases and treatment of missing values.
*   The possibility to translate the results into a Scorecard.

The grouping is performed according to two considerations:

*   Statistical consideration 
*   Business consideration


After grouping into categories in each feature, a statistical test for the significance of the categories will be performed.
And after dividing the feature into categories, the relationship between the risk characteristic (feature) and the failure component (target) will be examined.

There are several types of connections that can be found:

*   **Straight relationship** - the chance of failure increases with the increase in the value of the risk characteristic.
*   **Inverse relationship** - the chance of failure decreases with the increase in the value of the risk characteristic.
*   **Bidirectional relationship** - as you move away from the average value of the parameter, the chance of failure increases / decreases.
*   **There is no relationship** - no clear trend is identified - a uniform distribution of the chance of failure at each value of the risk characteristic.

### Segmentation
"""

# Group the DataFrame by the "target" column and count the occurrences of each value
result_counts = df.groupBy("target").agg(F.count("*").alias("count")).orderBy("target")

# Extract the counts of each value
ld_count = result_counts.filter(result_counts.target == 1).select("count").collect()[0][0]
nld_count = result_counts.filter(result_counts.target == 0).select("count").collect()[0][0]

# Plot the counts using Matplotlib
plt.bar(["Liver Disease", "No Liver Disease"], [ld_count, nld_count])
plt.xlabel("Result")
plt.ylabel("Count")
plt.show()

# Print the number of patients in each class
print("Number of patients diagnosed with liver disease:", ld_count)
print("Number of patients not diagnosed with liver disease:", nld_count)

"""Gender distribution check"""

# Group the DataFrame by the "genderindexcolom" column and count the occurrences of each value
gender_counts = df.groupBy("gender").agg(F.count("*").alias("count")).orderBy("gender")

# Extract the counts of each value
male_count = gender_counts.filter(gender_counts.gender == 0).select("count").collect()[0][0]
female_count = gender_counts.filter(gender_counts.gender == 1).select("count").collect()[0][0]

# Plot the counts using Matplotlib
plt.bar(["Male", "Female"], [male_count, female_count])
plt.xlabel("Gender of the patient")
plt.ylabel("Count")
plt.show()

# Print the number of patients in each class
print("Number of patients that are male:", male_count)
print("Number of patients that are female:", female_count)

"""Grouping both distribution together"""

# Group the DataFrame by the "gender" and "target" columns and count the occurrences of each value
gender_result_counts = df.groupBy("gender", "target").agg(F.count("*").alias("count")).orderBy("gender")

# Extract the counts of each value
male_ld_count = gender_result_counts.filter((col("gender") == 0) & (col("target") == 1)).select("count").collect()[0][0]
male_nld_count = gender_result_counts.filter((col("gender") == 0) & (col("target") == 0)).select("count").collect()[0][0]
female_ld_count = gender_result_counts.filter((col("gender") == 1) & (col("target") == 1)).select("count").collect()[0][0]
female_nld_count = gender_result_counts.filter((col("gender") == 1) & (col("target") == 0)).select("count").collect()[0][0]

# Plot the results using Matplotlib
plt.figure(figsize=(12, 8)) 
labels = ['Male with Liver Disease', 'Male without Liver Disease', 'Female with Liver Disease', 'Female without Liver Disease']
counts = [male_ld_count, male_nld_count, female_ld_count, female_nld_count]
plt.bar(labels, counts)
plt.xlabel("Gender and Liver Disease")
plt.ylabel("Count")
plt.show()

# Print the number of patients in each class
print("Number of male patients diagnosed with liver disease:", male_ld_count)
print("Number of male patients not diagnosed with liver disease:", male_nld_count)
print("Number of female patients diagnosed with liver disease:", female_ld_count)
print("Number of female patients not diagnosed with liver disease:", female_nld_count)

"""From the analysis above, we can see that there are much more males than females in the data.

We want to check if there is an impact on the prediction if a patient has liver disease or not for each of the features

### Boxplots for each feature and the impact of spliting the gender into female and male
"""

# Box plot for age 
sns.catplot(data=df.toPandas(), x="target", y="age", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs age and gender")
plt.show()

# Box plot for TB 
sns.catplot(data=df.toPandas(), x="target", y="TB", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs TB and gender")
plt.show()

# Box plot for DB 
sns.catplot(data=df.toPandas(), x="target", y="DB", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs DB and gender")
plt.show()

# Box plot for AAP 
sns.catplot(data=df.toPandas(), x="target", y="AAP", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs AAP and gender")
plt.show()

# Box plot for SgptAA 
sns.catplot(data=df.toPandas(), x="target", y="SgptAA", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs SgptAA and gender")
plt.show()

# Box plot for SgotAA 
sns.catplot(data=df.toPandas(), x="target", y="SgotAA", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs SgotAA and gender")
plt.show()

# Box plot for TP 
sns.catplot(data=df.toPandas(), x="target", y="TP", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs TP and gender")
plt.show()

# Box plot for ALBA 
sns.catplot(data=df.toPandas(), x="target", y="ALBA", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs ALBA and gender")
plt.show()

# Box plot for A/G ratio 
sns.catplot(data=df.toPandas(), x="target", y="A/G ratio", hue="gender", kind="box", aspect=2)
plt.title("Boxplot for target vs A/G ratio and gender")
plt.show()

"""From the boxplots above of all the features we can see that there isn't a significant impact of the gender feature. 

Therefore, we will keep the data as is and won't split it into female and male.

## Univariate analysis of explanatory variables

In order to determine which variables will enter the ML model, two main parameters in each variable were tested in a one-dimensional analysis:

1. The amount of observations in each category.
2. The failure rates in each category.

According to the two parameters above, the categories were built in each variable and they were grouped/united so as to contain the amount of observations and the amount of essential failures.

Variables in which there was a good distinction in the failure rates, i.e. a continuous decrease/increase in the failure rates between the categories and a non-negligible amount of observations, were then tested in the ML models to examine their significance and the degree of correlation between them.

### Features histograms
"""

plt.figure(figsize=(30,30))
df.toPandas().hist(figsize=(15,15))
plt.title("Features histograms")
plt.show()

"""### **Age** one-dimensional analysis"""

# Define age bins
age_splits = [0,10,20,30,40,50,60,70,80, float('inf')]
age_labels = ['0-10','11-20','21-30','31-40','41-50','51-60','61-70','71-80','81-90','90+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=age_splits, inputCol="age", outputCol="age_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert age group to string type
df = df.withColumn("age_group", df["age_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'age_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_age=df.groupby('age_group').agg(count('*').alias('amount of observations'),\
                                   sum('target').alias('amount of failures')).orderBy('age_group')
df_age.show()

from pyspark.sql.window import Window

df_age.withColumn('failure rate', func.round(((df_age['amount of failures']/df_age['amount of observations']) * 100),2))\
      .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show()

"""We can see that there is no clear trend in the data that can identify a the relations between age and the target value.

### **TB** one-dimensional analysis
"""

# Define TB bins
TB_splits = [0.4,1.2, float('inf')]
TB_labels = ['0.4-1.2','1.3+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=TB_splits, inputCol="TB", outputCol="TB_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert TB group to string type
df = df.withColumn("TB_group", df["TB_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'TB_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_TB=df.groupby('TB_group').agg(count('*').alias('amount of observations'),\
                                 sum('target').alias('amount of failures')).orderBy('TB_group')
df_TB.show()

from pyspark.sql.window import Window

df_TB.withColumn('failure rate', func.round(((df_TB['amount of failures']/df_TB['amount of observations']) * 100),2))\
      .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show()

"""We can see a straight relationship between TB and the target variable. <br>
When TB is greater than 1.2 (not in the normal limit) the chances to diagnose liver disease are much higher.

### **DB** one-dimensional analysis
"""

# Define DB bins
DB_splits = [0.1,0.3,1.6, float('inf')]
DB_labels = ['0.1-0.3','0.4-1.6','1.7+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=DB_splits, inputCol="DB", outputCol="DB_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert DB group to string type
df = df.withColumn("DB_group", df["DB_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'DB_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_DB=df.groupby('DB_group').agg(count('*').alias('amount of observations'),\
                                 sum('target').alias('amount of failures')).orderBy('DB_group')
df_DB.show()

df_DB.withColumn('failure rate', func.round(((df_DB['amount of failures']/df_DB['amount of observations']) * 100),2))\
      .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show()

"""We can see a good distinction in the diagnostic liver disease rates.<br>
There is a clear increase in the diagnostic of having a liver disease whith the increse of DB levels and a non-negligible amount of observations

### **AAP** one-dimensional analysis
"""

# Define AAP bins
AAP_splits = [63,147, float('inf')]
AAP_labels = ['63-147','147+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=AAP_splits, inputCol="AAP", outputCol="AAP_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert AAP group to string type
df = df.withColumn("AAP_group", df["AAP_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'AAP_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_AAP=df.groupby('AAP_group').agg(count('*').alias('amount of observations'),\
                                   sum('target').alias('amount of failures')).orderBy('AAP_group')
df_AAP.show()

df_AAP.withColumn('failure rate', func.round(((df_AAP['amount of failures']/df_AAP['amount of observations']) * 100),2))\
      .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show(100)

"""We can see a straight relationship between AAP and the target variable. <br>
When AAP is greater than 147 (not in the normal limit) the chances to diagnose liver disease are much higher.

### **SgptAA** one-dimensional analysis
"""

# Define SgptAA bins
SgptAA_splits = [10,36, float('inf')]
SgptAA_labels = ['10-36','36+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=SgptAA_splits, inputCol="SgptAA", outputCol="SgptAA_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert SgptAA group to string type
df = df.withColumn("SgptAA_group", df["SgptAA_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'SgptAA_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_SgptAA=df.groupby('SgptAA_group').agg(count('*').alias('amount of observations'),\
                                   sum('target').alias('amount of failures')).orderBy('SgptAA_group')
df_SgptAA.show()

df_SgptAA.withColumn('failure rate', func.round(((df_SgptAA['amount of failures']/df_SgptAA['amount of observations']) * 100),2))\
         .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show(100)

"""We can see a straight relationship between SgptAA and the target variable. <br>
When SgptAA is greater than 36 (not in the normal limit) the chances to diagnose liver disease are much higher.

### **SgotAA** one-dimensional analysis
"""

# Define SgotAA bins
SgotAA_splits = [10,33, float('inf')]
SgotAA_labels = ['10-33','33+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=SgotAA_splits, inputCol="SgotAA", outputCol="SgotAA_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert SgotAA group to string type
df = df.withColumn("SgotAA_group", df["SgotAA_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'SgotAA_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_SgotAA=df.groupby('SgotAA_group').agg(count('*').alias('amount of observations'),\
                                   sum('target').alias('amount of failures')).orderBy('SgotAA_group')
df_SgotAA.show()

df_SgotAA.withColumn('failure rate', func.round(((df_SgotAA['amount of failures']/df_SgotAA['amount of observations']) * 100),2))\
         .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show(100)

"""We can see a straight relationship between SgotAA and the target variable. <br>
When SgotAA is greater than 33 (not in the normal limit) the chances to diagnose liver disease are much higher.

### **TP** one-dimensional analysis
"""

# Define TP bins
TP_splits = [2.7,6.0,8.3, float('inf')]
TP_labels = ['2.7-5.9','6.0-8.3','8.3+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=TP_splits, inputCol="TP", outputCol="TP_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert TP group to string type
df = df.withColumn("TP_group", df["TP_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'TP_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_TP=df.groupby('TP_group').agg(count('*').alias('amount of observations'),\
                                   sum('target').alias('amount of failures')).orderBy('TP_group')
df_TP.show()

df_TP.withColumn('failure rate', func.round(((df_TP['amount of failures']/df_TP['amount of observations']) * 100),2))\
         .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show(100)

"""We can see that there is no clear trend in the data that can identify a the relations between TP and the target value.

### **ALBA** one-dimensional analysis
"""

# Define ALBA bins
ALBA_splits = [0.9,3.4, float('inf')]
ALBA_labels = ['0.9-3.4','3.5+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=ALBA_splits, inputCol="ALBA", outputCol="ALBA_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert ALBA group to string type
df = df.withColumn("ALBA_group", df["ALBA_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'ALBA_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_ALBA=df.groupby('ALBA_group').agg(count('*').alias('amount of observations'),\
                                     sum('target').alias('amount of failures')).orderBy('ALBA_group')
df_ALBA.show()

df_ALBA.withColumn('failure rate', func.round(((df_ALBA['amount of failures']/df_ALBA['amount of observations']) * 100),2))\
         .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show(200)

"""We can see a straight relationship between ALBA and the target variable. <br>
When ALBA is greater than 3.5 (not in the normal limit) the chances to diagnose liver disease are much higher.

### **A/G ratio** one-dimensional analysis
"""

# Define A/G ratio bins
AG_ratio_splits = [0.3,1.0,2.0, float('inf')]
AG_ratio_labels = ['0.3-1.0','1.1-2.0','2.1+']

# Create Bucketizer object
bucketizer = Bucketizer(splits=AG_ratio_splits, inputCol="A/G ratio", outputCol="AG_ratio_group")

# Apply bucketizer to dataframe
df = bucketizer.transform(df)

# Convert A/G ratio group to string type
df = df.withColumn("AG_ratio_group", df["AG_ratio_group"].cast("string"))

# Replace null values with "Unknown"
df = df.fillna({'AG_ratio_group': 'Unknown'})

# Show DataFrame
df.show(5)

df_AG_ratio=df.groupby('AG_ratio_group').agg(count('*').alias('amount of observations'),\
                                   sum('target').alias('amount of failures')).orderBy('AG_ratio_group')
df_AG_ratio.show()

df_AG_ratio.withColumn('failure rate', func.round(((df_AG_ratio['amount of failures']/df_AG_ratio['amount of observations']) * 100),2))\
         .withColumn('observations rate', func.round((F.col('amount of failures')/F.sum('amount of failures').over(Window.partitionBy()) * 100),2)).show(100)

"""We can see that there is no clear trend in the data that can identify a the relations between A/G ratio and the target value.

## Correlations

According to the methodology for developing statistical models, extremely high correlations should be avoided.

In order to verify independence, a correlation analysis is performed between all the explanatory variables, which were found to be significant.

A comparison is required with the values of the original risk characteristics.

The correlations between the explanatory variables in the model are at reasonable levels (lower than 40%) and do not require special attention.

A correlation above 40% would have indicated a high match between variables which would have required reference.

### Heatmap correlation
"""

plt.figure(figsize=(12, 8))
sns.heatmap(df.toPandas().corr(), annot=True)
plt.title('Correlation Heatmap')
plt.show()

"""We can see from the above heatmap that there are 4 couples that have correlation above 40%: <br>


*   TB & DB
*   SgptAA & SgotAA 
*   TP & ALBA
*   ALBA & *A*/G ratio

## Pairplot correlation

### TB & DB correlation
"""

g = sns.FacetGrid(df.toPandas(),col='target',margin_titles=True)
g.map(plt.scatter, 'TB','DB', edgecolor='w')
plt.subplots_adjust(top=0.8)
g.fig.suptitle('Scatter plot of TB and DB')
plt.show()

sns.jointplot(x='TB',y='DB',data=df.toPandas(),kind='reg')
plt.show()

"""We can see that there is a linear relationship between DB and TB as the DB rises the TB rises as well. <br>
The marginal histograms are both right-skewed as most values are concentrated around the left side of the distribution while the right side of the distribution is longer. <br>
Outliers are the data points that lie far away from the rest of the data values, in the graph we can see outliers in the scatterplot as well as the histograms.

### SgptAA & SgotAA correlation
"""

g = sns.FacetGrid(df.toPandas(),col='target',margin_titles=True)
g.map(plt.scatter, 'SgptAA','SgotAA', edgecolor='w')
plt.subplots_adjust(top=0.8)
g.fig.suptitle('Scatter plot of SgptAA and SgotAA')
plt.show()

sns.jointplot(x='SgptAA',y='SgotAA',data=df.toPandas(),kind='reg')
plt.show()

"""We can not really see the relationship between the variables. <br>
The marginal histograms are both right-skewed as most values are concentrated around the left side of the distribution while the right side of the distribution is longer.
Outliers are the data points that lie far away from the rest of the data values, in the graph we can see outliers in the scatterplot as well as the histograms.

### TP & ALBA correlation
"""

g = sns.FacetGrid(df.toPandas(),col='target',margin_titles=True)
g.map(plt.scatter, 'TP','ALBA', edgecolor='w')
plt.subplots_adjust(top=0.8)
g.fig.suptitle('Scatter plot of TP and ALBA')
plt.show()

sns.jointplot(x='TP',y='ALBA',data=df.toPandas(),kind='reg')
plt.show()

"""Observing the scatterplot above, there seems to be a positive relationship between the columns TP and ALBA because if the values of one variable increase so does the other.

The strength of the relationship appears to be moderate because the points are scattered in the graph.

### ALBA & A/G ratio
"""

g = sns.FacetGrid(df.toPandas(),col='target',margin_titles=True)
g.map(plt.scatter, 'ALBA','A/G ratio', edgecolor='w')
plt.subplots_adjust(top=0.8)
g.fig.suptitle('Scatter plot of ALBA and A/G ratio')
plt.show()

sns.jointplot(x='ALBA',y='A/G ratio',data=df.toPandas(),kind='reg')
plt.show()

"""Observing the scatterplot above, there seems to be a positive relationship between the columns ALBA and A/G ratio because if the values of one variable increase so does the other.

The strength of the relationship appears to be moderate because the points are scattered in the graph.

## Conclusion of final features

From the above jointplots and scatterplots we find direct relationship between the following features:

*   TB & DB
*   SgptAA & SgotAA 
*   TP & ALBA
*   ALBA & A/G ratio

Hence, we can very well find that we can omit one of the features. We are going to keep the following features:

*   TB
*   SgptAA
*   TP
*   A/G ratio

Therefore, the final features that are going to be tested in the ML models:

*   Age
*   TB
*   AAP
*   SgptAA
*   TP
*   A/G ratio

## Multidimensional analysis & Model Selection

We will test several ML models:


*   Logistic regression
*   Random forest
*   Gaussian Naive Bayes

We want to choose the best suited model for our particular problem.
So we will choose the model with the hightest AUC.

Recall calculates how many of the Actual Positives our model capture through labeling it as Positive (True Positive). Applying the same understanding, we know that Recall shall be the model metric we use to select our best model when there is a high cost associated with False Negative.

In healthcare, sick patient detection if a sick patient (Actual Positive) goes through the test and predicted as not sick (Predicted Negative). The cost associated with False Negative will be extremely high if the sickness is contagious.

We don't want to predict that a peitent doesn't have liver desis when he/she actualy has. So we will choose a the model that gives us the highest value of recall.

![confusion matrix.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAmEAAAFACAIAAAB7jzwSAAAn2UlEQVR42uzd32sbZ9738Us3Oc6yPTGhce20UmyMDtYmLM2IuA3ZDZFiiGnBNjkJyY1H8LAPEqTuDSaYYIzhThpuiS0PeMwm+CTYhqfI4GhC2pDWxZNSgr0H2qBYShPHKcUnXZJ/QDfzQ6ORLMmSLFu28n5hiD0azS9N5qPvdV0aHcpkMgJodi6Xi4MAoFqHOAR4R/B2EEC1763/g6MAAEBRZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAICMBACAjAQAgIwEAICMBACAjAQAgIwEAICMBACAjAQAgIwEAICMBACAjAQAgIwEAABkJAAAZCQAAGQkAABkJALVLR30ul8vli6bfjfWCjATQPNSgK5/PF1TTu7263VwFyEgA2CWapgQ8u1V+pavOUl4QMhIAGkyOZ3SpuGwGZfjmrqST2z9lrGfK7+aYg4wEcKC4/SMRyfgtsZa2y7mguqV3Lx0N+nLNs8G8sjOtZh/zBaOp7epDfW6fvSA1bc4TUIwHlUBBp2Kt68WBcIhDAODAUQIe8xftaUoItx5iVoZZk5WwJyFSyyG3kYIe+0FNCQfKLjkd9XnCmmNBgZv9mf5Sc9dvvaCOBIBqpdWbZmRJA+fzWkPleMpsI9VzbULPIjliTDHaZ6Vc66waUxxPSFlVaanQs9ZmLd1ckvBPZTJWm6/ZBGyGYB3Xi/0qA7wDONUPECuM8klW92R+VFnRVCp+JD28rCdI2SDLzp6dkLfAgseKbJZjxTtbLw7EdYM6EsD+JklyPLWs14s74T1eyZic1FOt8pnruF7Q1goAVbALtuXlysacbi3QzBZRkzniZzueLqnymeu4XpCRALAr3Me9wvp0iD3IVY0GfebgUyv2tHmzlzAdveQYkrPdotJqtGCoqiPz6rhe7Fu0OIP+SOzH/khnj+N2jxbvGSzocSz/aHaBWxdVbDHWtJ2sF/RHAsCeVJKh5VRclnKBZfZhWk2e/qlURLYek+RIquwQU31R9tzGcmayixmJ29O9x+u9XuxPLjMqgSY/0V2c6gCqvm5QRwIAUBwZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAZCQAAGQkAABkJAADISAAAyEgAAMhIAADISAAAyEgAAMhIAADISAAAyEgAAMhIAADISAAAyEgAAMhIAABARgIAQEYCAEBGAgBARgIAsBcOcQjwjnC5XBwEAGQkUEQmk+EgoOibJ84NlDo3aGsFAKA4MhIAADISAAAyEgAAMhIAADISAAAyEgAAMhIAADISAAAyEgAAMhIAADISAAAyEgAAkJEAAJCRAACQkQAAkJEAAJCRAPajdNTncrl80XSd50XznxUH53wgI4GGXUhcQbVwWt6UXVtzXdaSvjevCSENnHfndij/sqcG7Qnu8wOSENr8PUKyhvPEVvsLV/513+2zwuLzBVXzBDg45wMZCTSOEtj1TNRzalferuddDHO08M2iu0RI1kqKpDKmuKwE6vNi7vVZkT05NCXgMU/5A3M+kJFAo659siwJZWKPm5vcoeVMZspfp4uh8B53XgwlSRKldsl93CsIyR3xT8XlWo9g+dd9V88KIceNiE9FJOONoXl+HJTzgYwEGqVrZCYilaq7rJbKrU1sjsl5jZuO6dlJ6ajPFVD02s6TW4o+n/HbllrCfqTM2m2pp/rFUOryOCd6r12TS5aSni79Eqk9TfHK18w8hnYiOZsyna+S44G8F9T4Y8/PivymhIN2PpCRQOO4Q9fk4nWXGnQFRDzXxGZdtvTJCavtLS4b7XDLIbdx3ZvosprkUhERNhu09OrAmkt/oKBO8PfnFyVqTBFyv7/M2p2X57WE2FowCOEf0YsFJaaWKiRFYo1CsmZmBmVz0BP2xp3NsFZsqcHcA3Gx5exqwFlhPqiaRWb24QNyPpCRQCPpmbK17kpHJxQpMuJ3zGNctvRrkN3XY1zNrHfh7tCykZX2taeSK49zAc6LYam1b1WkYDCLhaK5n18EoVrpqC+gCPma8UKrN8OaHLcDzmiGNY+6FVTZ6fZ5UekZWf+zQgnohacnEDaKTHsZB+N8ICOBfVBKFozd0csFqynMuLqEtVz82Vcm/fLluBzlmsECShWXw2zJ56gXSq29ih0SpZuQUSX7pfCERSRl1X3Gu6W8LNITx8g264yq+aMVu3JWmOEpyfFUtZlNRgKUksbYnYJ+mdxwRoN5aTEuhNaFKqDI8ewFx9kGazSkVXs5VGOOIqHU2rdevot2JpVpb0X1HC9FhfninzJPA+NMqT4o635WWGN2MsvLU373QTv8ZCTQ+FLSGLszMb+1JiiQvjevZS84zq4ko6QsPt6+fIOWHmf65VC/GNrPL7X2KktJZWIiIQqrY9SxwMx7jfSja5V8uaRMlRgU1oizYouDcT6QkcB+SMlrstA0+5JhdOo5+vTUYG4Yodm5kz+C1Xn9UoNb2lrLXNr0NSmx6FrCEbHl1m4/r/yAC6OUdOyQ2G5AB2o4YxxN9Marbkak8+XSY6jEKNMGnBUFb/gOxvlARgL7gdk+6bgELhvDU60onOhKGTWjfg1ytHfZI1iNStQKz4CIOBeV650qPlrfuByGw8JZhZZYe5FKpOR11tiiYmVDqSs2qj1jpszBrFYfdMLuqvRPxYU92dEeXyRi9/qsOJDnQwZ4BzTHqZ6KSPldQo4h/A3ZmGrWXu38nBsH9RSt7FXet+dDwblBHQkcKI4B9+noRMluyD1o7avuZmLlb1KGZmkDrvSsODDnAxkJHJwLUGjZGqyY+yxA44bSVxWSRCQheTDPB5dZTgLNzeXiVAfnBqo+N6gjAQAojowEAICMBACAjAQAgIwEAICMBACAjAQAgIwEAICMBACAjAQAgIwEADSNdNRX6suyyEgAAEBGAgBARgIAyjObWI0vW9OEEErA+u41Gl3JSOBdvRxyDQTISABAee7QcsaQikhCCDlu/pWZ8nNshDjEIQDercthiMMAUEcCALAzrkwmw1FA85/oLk51cG6g6nODOhIAgOLISAAAyEgAAMhIAADISAAAyEgAAMhIAADISAAAyEgAAMhIAADISAAAyEgAAMhIAABARgIAQEYCAEBGAgBARgIAQEYCAEBGAgAAAAD2u0NCiEwmw4HYSy6Xi2O+98dce7nIccBWUntf5pnMcUCR60aHQlsrAADFkZEAAJCRAACQkQAAkJEAAJCRAACQkQAAkJEAAJCRAACQkQAAkJEAAJCRAACQkQAAgIwEAICMBACAjAQAgIwEAICMBACAjAQAgIwEAICMBACAjAQAgIwEAICMBACAjAQANMJGsENxdSyp7+juv40OKa6Oheh6Tc9eT/g6FNdQIk1GonLpqM/lcrmCauG0oLoXaw6qvAR4l6ljiqsj7ye4tGdhk/vxDS2pOwmeUrFd/tGq1/VqflWI7o/Ot5U5dKV3re2DgW4hVp/fWycjUS0lsOtppQZdLl80zbEGclm1trYvtkNbTQbO1iHJ0jMLeizNvN2Vt9XfP9eEkPwfuKs5dI5dO3ze3yLE5vz3u7J5hzidm5Yky0JRJqIj/pB779bqDi1nQhx8QAhpdGj50uG9X688LU/1GtXeWU0TyYmZP/mr3Yw27/Izb42PVvl+4p66KYTwHjtcyaErumvuY38UYlNTX6Uved3UkahY18hMRNLCN9XSFaDFWW06JhuyRaJjenZSOupzBRQhtLAntxR9PuO3LRWm/UiZtQPNTJ1Z8NkNhkNLRXvg0ksJn9WuuBBceuso5pbs5/rGNrZvuzEbIR1p5Fy7byzXgVdsjXYX6dvokOKZ1GNMm5zNVpN5j+a1JOf3DlawzW+ergohWrraqorw/F1re08SQqz+ntqFl4yMbGru0DVZKBNFmkLVoCsg4hlDXFYCVpjpkxORlDVZCCmSWtar0HTUN9FlTs6kIiLsMXJNrxmtufQHpvx5q/D3y0Kbv2evW40pQu73l1k70Nw2YpObmv3XajL8X1tGmqwnLg1r2qr5x6Yy/E/VCpsFz2TSfq42p3rGNsqvLL1k9PNlSzR1bDbgWLs2p3nMJCuxxsqY7ZxC+c7aGGfDaUXbvP4mof/zx+PVZGTBrom2PxhV7b/X1slIVMk/UqyUTEcnFCky4nfMY4RZei0hpIHz7lzGPU1ZYbu8nG2xdR/3CpFY2z7VnAtwRmSptQPNxSy8XLnhLa0j0/7UMznzTM48kOSipc/673qodEv6bA/8Eata2ripV3ItkQfmc/1ytxBzL0qFmTKsr9QzrJmLGjEaJyfm9HItMj1krt0ovLSbS6XWmEvB0KycGm0x2z8zz+SCJlD3px/pi7I2xmw4bRn49HB129z9nmebQ1d613R/6OrerReRjHw3SsmCsTupp1q2gVTnCWu5+LPzSg81qcvjqDwtAaXSgO6XhRJTC6vIUmsHmt+LS2ar5lmt+H+j3mN6lqxqng7F9483x/+7V/8/s/TCmHkzfNZ8rqqsVlI2tUiD/tSs0UVnBeFH53vNwst7bVD/N/HibfE1Vs5q9kzGlvJHqNa4zRVy7NouIyPfjVJSKBPRgjesVgNpllkmerokO74CihzPFo/ONlijebXakFRjjtKx1NqBpmIWXsaPETzriUvDyWyrZimtU7ND8dFOqVtoc1rgbC2fGpSnzZVeWB5vreA/1g7XmGtuzR+hWo0t9XThoatx18hIVFZKGmN3JubtKUYUPt3awZ2+N6/JcTu57A5Go6TMtsHm0xdVPp/1kNQj0n5+qbUDTc2MEDHoz7W1brW05Bt747nUuzwrxwezH2kwx6SITqvd0vq5EKq8D88a1ZL9EGG26XXg08PF11iM9vxN8QtMtrn1Zq6hVdRhm6tgDvwhI1F7Sl6ThabZbZru8wOScyyPGswNLlUCWwe1OlNNDW5pay0TePqalFh0LeGI2HJrB5r3v+GxP+r/zKnl2lrNsS1GV1xgLvuhCKtpNGm1W1rDYqu5s4zVImq3fBoXg8EeM7GKrLH0lhf5lGS2uVVx3Aqg0m2uy3Cbmgb+kJEoLOjyQnPZGJ5qReFEV8qoGd3Hvc5WUHsEq1GJWuEZEBHnoqz+zpKf4TBCMhwWziq0xNrRtH6b+6xPav9ibkfXwQW5vU/6bOH1wT0Mvb1GoaaTBqXIYNF5/mTPI7pb5OmhKWNYin98KD7YItW+7sOh2aGIvWSzP2+8tcwaC7Yq+9yWrQlqN7eK/IbWyrbZHG6z+XRH54bZ21o48KcuXEKITCbDf+K95HK59ucxT0d9nvmBVK5z0OqGbILeQpfLpb1crNPCntxovx4rMv3crZd/O1nvLX99+4uB8aS4eF2bPOGInOFop3PKLgVbndayviB/Mp3oGZ7/5v27JQ/d568+G46uCCE6Qz98NdjmONT6Ey8cNbdnxflofUjtfZlnMtelhl12ZhY8k5s7ud/CzpdQ8rrRoVBHoqChJ/cxjHR0omQ35DvtxJcvFzXj59ZFoaeX9Wf9AzLn7vUbj3Z5tx59vdNSr1TMP/wxIYS3789HK5o9GZ16Umz6EV9fpxDJhw9/4xRsJmZ3pqa+qvXzX84PnNQfGQnHyRpajsu5T2V4wqIpasiDr+dcf4+I/X2PmxmPDH6zWI9S9bflxaTRp3Uk9/bih2GjF6oz9EPB24tOb48Qd2eLRvXRY+1CiMTiz685JZrJDm9Knn9LdDISu8s/xUcyduTxaJ98+7fHo31Se580+sScYv5iz1D4Z7v58/Xjkkttvfg/w96V6buPSq602EKe3Gi3p+s/8u3fCqdnu/de3/5Cunxfr+E+0aebNau9qfoveR2B+hLsuna7Xfh13WhBbfuwkuPXfvn/nitZSn7YqifrysYG51lTORya3cGQ1zbv8jM5s2uflSQjgTpLjA/fcU/r5dF2Rdjj0b6r4nq22fb+1TIDUtouXL5YvJQssZAnN9qvp8eMzXh5vV8I79i0cuWI0cs422ZVb9MhMT1gpODRK19pd87Zhd2Xp/NWcfKv58TKj8v22/xHP8XEuVOnK9uF9V+NNrT2Dyq8Ap7+PKSXkj8Vidu2943r4MtX65xl2CNkJFBvPcNjehptZ33hzt3OUNDK0ZPBYa8zh7YwZthSSpZaiJ5MnWfOmJtx4tRFkUj/avx+ZPAbe8zLkQ86hUj+un3T5emP+0Vy/ZdsKn97X1z8+GRVu9DT2lrp4TP7He/fub213/H9th5OL+wpvhsLqLfO9ysanPLLRkIkE5/0RR3PPFNmfr2UnL56+etTLz/ffiF6yZV8+PC3QT2tn/x4V3jH3s/OkD8ut6LU0VP26rdPvjx9wlxa/50TtexCZY5eGeofvx4b//+Pr3zM2QQyEnh347SqTzLohdrd6Tu3Pz6z/UL0kis2PiyNG39dvK5ZpW22Ddb48/Fo39VkZav+6zlx+afHkydOPvop1jM8f7rKXTA6EY9WuqMnLo51xsbv//iIjESD0dYKNMiHrV5HA2ZF2i6MjXUmxmcfbruQ9Z8frpy7lf2MSq5n9NFPsVwb7NbtKe3056Ge+z8+Eo+/vZ/7FEcNu1B5KSlE7O+z+Z8HMIf/AGQk0Cxa3Z32hxn0uu2uHXh/PpP3iY4nN8oNbXWGRzKxUslC7l/dOqjVmWqPvs5tjKVM4B3x9XXGvl14lXREbCW7UONAG72UFCvJRF7wVzn8ByAjgX3u6JWvbl20PlNxVVyfH+u0U2fwG2NkqRVjs20/VHIXAiM8HNFVfCF6MtmfPlzUfhgW48M3HlmVaOyyMfNlEXIuyhw6ezn32Y/CHTlzynt3OipO+dpENbtgDrSputw8euVqqKCv9JeNRHXDf4Cd4l50jTjo+/VedM19zOt3L7oD4PXtLwYWTxm3cLMrvOtp6+MfjdiY8aR3x2uv13IKcC86lLxudCiM2QGa1MqPy+sXzNE0r2/PxkRn6MyRxlTSZ055x5OJxZ9fX7lwtPbFmPfrKdGZ2mw2gh1qsS8G6Yw/6929LwAwb3wqBv2Z8YJa/W10aDac//1Tu3F/1H2IjASa0NErX91K913NfSqj/rcCr0Lbn8/0TCccmV2L9Z8frgjR42zpBchIADU5ObmoTe6TbTky+M3i4E6D9oLy8sI78+q1Tj2Tp4wvQ/ad1TTREnmwS99OXB15Wi7yzVlNjTE7AHBQvI0OKa4OJbhkl9cJn/XdxRtB85elJZ/1hcZLUcdY4vRMdnqH4hvbSNdne6yVqubCHZvhmKJvtjqz4Fi7/U3LRWemjgQA1OLweX9LeHVT+W5jqrdVT77vn2vWNxu/0R9f1TzD2XlXk+Gz4rjRf2l1NGZpc6pHbO103J4yrFi9pN1Syr6N+KoWyO+qLJiijs0G5oRj7ZpnTZR7OnUkAKAG5rctirkVo0Ys8tWJ0qg/9UzOPJD02UQyplecGzf1gGyJPJAz+kN+uVuIuRdqHTdr0Fip88s37CnriQk9IFsi00MZe8NWtZtLZZ9ORuJgSUd9LltQ5YAAjdHmvTYohNic//5tsa9ObBn4tNVtfymjaemFUfxths8arZ1nVUWv2/69Vv3Xp8jTRsoW5llL5D9b8+PNMWX9d00YG9l72LH9IvHibemnk5EAgJr4/9IpjC/uV3MNrdgt9EeiIu7QcibEYQD2gd5jskgqVjdeXkOrUV9unL/U6jZLTNHSpZeY70lCaKIz8qC3AYNj24y1rz6/t+7V155tejU2+w0ZCQCor9aR0RbFHIOT19Cq0yZVj/2ZH+tR77VBLTCXDJ9Nhu35nINuKpYbs2PdQ6CSjPxgoFvTVjfDZ5Xc2gd7Qgfkc660tQLAAWON3CnS0Noij3ZKVgp2xrMp6B8fig+2SI3Z2MOh2aHIYIu9hdKgPzV+YO65y/1aG3HQuV9rI475O3W/VlTuYN6v1bxZnfPeAlunYMfXjQ6FOhIADpj0zIpSrKEVdUdGAsDBYn4skhGte1JK0tbagINOW2sjjjltrSiK78ZCyesGba0AAJRCRgIAQEYCAEBGAgBARgIAQEYCAEBGAgBARgIAQEYCAEBGAgBARgIAQEYCAEBGAgAAMhIAADISAAAysipq0KULqpwHAIB9lZHpqM9l8kXTvBIAADLSjsh785r1qzZ/L03pBwAgI62wuxnWhJAiEam6kAQAoMkzUo0pQghp4Pz5gRIhmVaDPrsxNqimzRoyoBgPKoFcG21haWm14WYbcNNqNLcgfVE07AIA9nNG2hHpdhcNyXTU5wkomt0YqwRuqrXWq4FwbkH6osIeWmoBABXK7Lm4rK9XiqT0P1JGc2v2L8fjQshxc464LMlx5yPWX0Un5S8vFY9E4qlU9qG8eYssbE9wygHAgXCosVWk/pdeSYY1Ta8kQyFjSnotYabciN+cwz+17K9xXW7/+ePRm5cmnLXk/nhfwsm3l1wuV+aZzHFAkXOjQ9FeLnIcsJXU3neoYREptLDHFXZMzoVk6qmRZ97j7jqsLOix+jABAKjKfzQsIrfSwlano6fLaC1NrKXrtbJsQ26cSgIAsH8z0kqt/E5AK7uUmBGS7uPebGYaKZlWowXDUZ3xaSWqEjNmTkcvhbe2qnqPm8uJUVICAPZrRqajE2ZE9ud1MPr780LSP2KOu1ECHpfL5fIEwloib0Yt7LE/3mGNjLVm9uQnZDZAzcdodQUA7N+MtLoaCyJyS0i6Q8upiCxZD0qSHJ8xh/MI/0jcnm5Wh8IdmrGnSXIkZQ1stR5bzs0vyfEIja0AgAq5GGPZgIPucnHMG3DMGdeKoucG41pRgtTex3djAQBQHBkJAAAZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAZCQAAGQkAAMhIAADISAAAyEgAAMhIAADISAAAyEigiWwEOxRXx5LKkch5Gx1SXB0L0fUdLGM94etQXEOJNIcTZCR2KB31uZyC6o6WVPLp5R9tEuqY4urI+wku7VmuFESLEcB7kBNmINUr6ddfza8K0f3R+bYyx3O7/W37YKBbiNXn99b5/w0yEjsnRVIZU1xWAi5ftA4XVjXoqs+CDlINtLbW2A3YDP9jY9ffVM0suDoU38zbXVn49881IST/B+6Kjmep/T183t8ixOb892/5zw0yEnXkn4rLQpu/V0u2uUPLmcyUv6ZHm+stx+hQ5pls/kz17tlqW6RuIeZWontcPLV5l/U97a3HS/v2nrophPAeO1zB8Sy3v+5jfxRCaOormltBRqKuPF2S8329syXW2VLqeMCerJeOxh/6gwFFCC3syc1gP7qlwrQfyf6x42bf/UadWfDZDYZDS0Uv6+mlhC/bhBhceuuo25bs5/rGNkpf9P947f90liklSy5nPREcym/SHNsosc1vo0OKZ1KPMW1yNltN2r2qVhNornk5v1+wgh1583RVD7+utkoOatn9bXtPP41Xf0/xPxpkJOop9VRz5KAn7I07m2Gt2FKDuQfiYqKwSVWvGeOy3YZbUD36+/NLVTWmCLnfbwVkQMRz62uSxtqN2OSmfVTFajL8X1u6CdcTl4Y1bdX8Y1MZ/qdq5cqCZzJpP1ebUz1jpVtTe/8U0UurF2qRgCy1nI3gWU1ZrWmbC5ktnEL5ztpCZ8NpRTuy/iZhhN/xtsqOa+n9FW1/8Or//HuNLkmQkaibdNQXUIR8LeTWA+tmWJPjdsAZzbCKkYfptYQj8qaWjdkrZ4Tk09SWiExHJxQpMmKt0D8SkWps9W0ws8Zy5UaytI5M+1Nma+EDSS5a36z/rudHt6TP9sCvX/qNoLqpF20tkQfmc/1yqUjIS6nkRGFnYenlLL1QhBCDxuaZ29YtpcZbS2zz4dCsnBptsds/ly/lNYq6P/1Ir96sLTQbTlsGPj1c3Y50v+fZ5nhuu79CiD90dfP/GWQk6nNRN9tEXS5PWERSVt2nB6HU5bxcebokM9vcoWuyUAKuWkfl6CGpxNTCKlIvYe1N0TdGa55D/OKS2Z55VlOKl0TH9NhY1Twdiu8fb47/t9G9ZwaY2AyfNZ+rGgVfudrIfalH1kPln3lZUmY5Zpvk2ot7Sxvq98/12Y7/wV3hNhep3owBpSIZW8ofoVr9jlSo+P4CZCTqKTeuNVNhTeifMptDjUirPihzIanGHKVj/qZUsTn77HDmxpgYUbeeuDSc1FbLP6l1anYoPtopdQttTgucrfkDgq0jep1npFRFqWa0Sa4mw8NqYHJTGjSLyAq3uVQhK5TvNvJHqFZjS5FdeDx3sr9AvR3iELzLBaZRNWb/0us8+Zo/PylHoj5P+KYaKhyyqhedidIhORKRPDF1SsQUaSDlzj1FyVtjUzDTQgz6M+OtYj0RLFqWLS35vjs2M967fKlXHVsIzG3Of/829Kle5GmiM/KgN9RWTWk1qSr/byU39qqt5HLSMyuKEPJ04fjbbbdZe/5GiMNF1v7pR9Lkpjb34ma33dBabgN2rsj+Cnv4D0AdiV1iNag6BpwGsq2iajA35lQPzvwm2YKILbH08wOSEouuJaSB827HNKvH01pjUwxtNT+HIObU8u2W2pzqMXrdAnPZzz+0ea8NCiGSVhOlNcR027EzRmm16hhxU3o55rYpw/b0BXPEaZltdj5U5FOS2eZWxXErgEp3pMaBNlv2t4bhPwAZiWr5p8zBrKZAwu6q9E/FhT1ZkePFWkRzfZbFg84IyXBY5CLSGA+bigi7R3KiK9UUH6js7Y0Ptpi/SoNSZLDoPH+y5xHdLfL0kFnY+ceH4oMtUtWl1elI/oiVkstpOybn/b2pzamXZt6W2+beP0Wsh1oKPshosJpbRX5Da2U7Yg602Xy6vtP9zY6Bes/Df+MD7Le5z/qk9i/maut3WF+Q2/ukzxZe7/JWuoQQmUyGl2svuVwujnkDjvkz+V3ba3VMCczl2lrNZl6ribURjI+IbEqjQwUjZhu1HOvc6FC0l4u7sLtPbrRfjxWZfu7Wy7+drPfKXt/+YmA8mfv74nVt8kSt0TUc7Sz19PKPVhlyn0wneobnv3n/bskD9fmrz4ajK7lJ3p5zl//nbyfbsluy0hn64avBXWtOkNr7qCOBJme3tZrNvPJfWhu1JeanR3Z8fxznx072sxNfvlzUjJ9bF43Qsv6sf0BaeobnrVVc7797vT411qOvay/1yof6wx8TQnj7/ny0mmclVu5f/eTrx/qvR3x9nUIkHz78bVdfRTISaFr+8aGI3cYrhOjujGSbeRujLrcjz78xOorH851zYuXH5VqO85HBbxZLl4nlH62iWl1eTBr930dybyZ+GDZ6rDtDPxS+mei/45zh/p3bei4ePdaup+biz6/JSAA1ORwav2DfDTUz2xvqPdzg7ZmVM88u7Gj4q3kL2VnvgR4f/Xi0T7792+PRPqm9Txp9Yk4xf7FnKPyz3fwxq6jtfNjqdRZtt7/IPj1vsVavnvFz41HhqvVnXb4vRDL6SW4G+1H9l7xS9cmNgoWU2+Bf11f0OGz7sKqX/s9nerbs48rGBhkJAE0mMT58xz2tl0fblWWPR/uuiuvZZtv7VytpRP1lI+EIyIHx9lvOZlgrJp/c+GTafceaLv5euNijV77S7pyzC7svT+c9evKv+aXqo59i4typ05Vt8PqvRnt7+wfVvFt6/ejnh0bfpFF9CtH2vvE+6eWrdTISAJpMz/DYlSPbz7a+cOduZyho5ejJ4LB320bU9QX58n1xccgYzPLk7niy/47dbmk0w96dnVu3gyo7/ZsLVXUNitMf94vk+i/ZIP/2vrj48cmqNrintcK+8dhlvSQduDydMI7bRSut32/r2fVXiXsIAEAjdL5fUSbpFWEy8Ulf1PHMM0XnXJkeaJ82f/WOTWtmAOtB2HnG2aT5YatX3Nez7fSFyxenr17ui4naRoeeOHVRXP32yZenTwjx5Me7ov/Oieo2uPpD5r04NDZ54ugevkpkJADs9zitKMN6huerrAVPTi5qk0bT6Cd90Rqe/tdz4vJPjydPnHz0U6xneP50lRtsdCVWssb+O4UtvXuGtlYA2Mf0si/XpFmT/KfrdZ7VcZhLyh+GvSvTdx8VXXtppz8P9dz/8ZF4/O393Kc46rDBFTIH/pCRANDsWt2dVjehOeblbvYBYzBnLDeg5smNCoe2Wk+/cPmiiF22n/LkxuVsx+Gjr3NjXPXgLDXKtEzgHfH1dca+XXiV7Dxz5kgVG1yX4TY1DfypFm2tANB4R698dStttHkad8mZH5sdsEbUHBn8Zlp8Npzta+wM/fBVVXchODm5eEv0XW2/b/6Z66o8/bdb3/ZJ7dZs/XcWi7SOttl9lsUbPI+eOeUdn472DM/nnlvJBhvDbVaM9K054cyxuxUP/KkN96JrAO5F15hj/u7diw4VnRu7dS86lGPePM87Nq1UMrh3d5awLam9jzoS2GMbwQ612NeDdMaLfIFi3Zj3OLVv1lrwp8Pb6NBsOP+bp+p1Z1QgvwBNJhZ/fn3lQk3jVM079TjaeHcH/ZEAgD1n3jSnxhvmCbFu3E+g55Rvl29JSB2JKkqRqM8T1uR4pim+06phWqeeyVP6f/KE76ymiZbIgwuh/Xfr0a3fzAzU1ZHBbxYHa4/YC8rLC3uwldSRwD7xNjqkuDqU4JL9Tjnhs76seCNo/rK05LO+wXgp6nj3nZ7JTu9QzG9R3gXWNqjmuhxb5Zii74U6s+DYGPublovODOx31JHAPnH4vL8lvLqpfLcx1Wt0GX7/XLO+yviN/viq5hnOzruaDJ8Vx43+S6tnMUubUz2iDt8QqQwrVqdpt5SybyC+qgXyuyoLpqhjs4E54dgYzbMmyj0d2N+oI7GtdNTnMnjCmn7xDJh/uYIqx6auzK9XFHMrRo1Y5FsSpVF/6pmceSAZ3/ifjOkV58ZNPSBbIg+Mb/Z44Je7hZh7sYsvzaCxDc6v3bCnrCcm9IBsiUwPZeztXNVuLpV9OkBGAthem/faoBBic/77t8W+JbFl4NNWt/0tjKalF0a1txk+azRvnlUVvVD799qOvwlBnra/UcuZZy2R/2zNjzfHlPXf9TdR3R+dN7+By9odkXjxtvTTATISB7y8CS1nDKmIXhfIcfMvRu7sAv9fOoXxTf1qrqEVQMPQHwnsJ73HZJFUrH67vIZWo77cOH+p1W2WmKKlSy8x35OE0ERn5EFv4wfHthkbs/r83rpX35hs06uxF294bUFGAtih1pHRFsUcg5PX0KrTJlXPZPYP61HvtUEtMJcMn02G7fmco2xqlRuzY91DoJKM/GCgW9NWN8NnldzGDPbsw0+2ABWirRWVMxtdaWLd5aNsjtwp0tDaIo92SlYKdsazKegfH4oPtkj7YtsPh2aHIoMt9gZLg/7UeCuvKQ4u7tfaiIPO/VobcswPzP1azZvVOe8tsHUK6nducL9WlCC191FHAvtLemZFKdbQCmDvkZHAvmJ+LJIRrcC+QFtrIw46ba0NOeZ8NxaKnhu0taIE2loBACiJjAQAgIwEAICMBACAjAQAgIwEAICMBACAjAQAgIwEAICMBACAjAQAgIwEAICMBAAAZCQAAGQkAABkJAAAZCQAAGQkAABkJAAAZCQAAGQkAAAAAKCZHMpkMhwFAAAK/Otf//rfAAAA//+3yBmk9wNRxQAAAABJRU5ErkJggg==)

## Prepare the data for machine learning
"""

# Prepare the data for machine learning by assembling the feature columns into a vector
assembler = VectorAssembler(inputCols=['age', 'gender', 'TB', 'AAP', 'SgptAA', 'TP', 'A/G ratio'], outputCol="features", handleInvalid="skip")

data = assembler.transform(df).select("features", "target")

# Split the data into training and test sets
(trainingData, testData) = data.randomSplit([0.7, 0.3], seed=42)

"""#### Logistic Regression"""

# Train a Logistic Regression Classifier model
lr_classifier = LogisticRegression(labelCol="target", featuresCol="features").fit(trainingData)

# Make predictions on the test data
lr_predictions = lr_classifier.transform(testData)

# Evaluate the model's performance
lr_auc = BinaryClassificationEvaluator(labelCol='target').evaluate(lr_predictions)
print("Logistic Regression Test AUC = %g" % lr_auc)

# Evaluate the model's performance
lr_evaluator = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="accuracy")
lr_accuracy = lr_evaluator.evaluate(lr_predictions)
print("Logistic Regression Test Error = %g" % (1.0-lr_accuracy))

"""Precision & Recall"""

# evaluate the test predictions using MulticlassClassificationEvaluator
lr_precision = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: "weightedPrecision"})
lr_recall = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: "weightedRecall"})
lr_f1_score = lr_evaluator.evaluate(lr_predictions, {lr_evaluator.metricName: "f1"})
lr_support = lr_predictions.count()

# print the results
print("Logistic Regression Precision: ", lr_precision)
print("Logistic Regression Recall: ", lr_recall)
print("Logistic Regression F1-score: ", lr_f1_score)
print("Logistic Regression Support: ", lr_support)

"""Confusion matrix"""

# Get the confusion matrix as a NumPy array
lr_confusion_matrix = lr_predictions.groupBy("target", "prediction").count().orderBy("target", "prediction").toPandas().pivot(index="target", columns="prediction", values="count").fillna(0).values

plt.figure(figsize=(5,5))
sns.heatmap(lr_confusion_matrix, annot=True, fmt="g", vmin=0, cmap="Blues", cbar=False)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Logistic Regression Confusion matrix")
plt.show()

"""#### Random Forest"""

# Train a Random Forest Classifier model
rf_classifier = RandomForestClassifier(labelCol="target", featuresCol="features", seed=42).fit(trainingData)

# Make predictions on the test data
rf_predictions = rf_classifier.transform(testData)

# Evaluate the model's performance
rf_auc = BinaryClassificationEvaluator(labelCol='target').evaluate(rf_predictions)
print("Random Forest Test AUC = %g" % rf_auc)

# Evaluate the model's performance
rf_evaluator = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="accuracy")
rf_accuracy = rf_evaluator.evaluate(rf_predictions)
print("Random Forest Test Error = %g" % (1.0-rf_accuracy))

rf_classifier.featureImportances

"""Precision & Recall"""

# evaluate the test predictions using MulticlassClassificationEvaluator
rf_precision = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: "weightedPrecision"})
rf_recall = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: "weightedRecall"})
rf_f1_score = rf_evaluator.evaluate(rf_predictions, {rf_evaluator.metricName: "f1"})
rf_support = rf_predictions.count()

# print the results
print("Random Forest Precision: ", rf_precision)
print("Random Forest Recall: ", rf_recall)
print("Random Forest F1-score: ", rf_f1_score)
print("Random Forest Support: ", rf_support)

"""Confusion matrix"""

# Get the confusion matrix as a NumPy array
rf_confusion_matrix = rf_predictions.groupBy("target", "prediction").count().orderBy("target", "prediction").toPandas().pivot(index="target", columns="prediction", values="count").fillna(0).values

plt.figure(figsize=(5,5))
sns.heatmap(rf_confusion_matrix, annot=True, fmt="g", vmin=0, cmap="Blues", cbar=False)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Random Forest Confusion matrix")
plt.show()

"""#### Gaussian Naive Bayes"""

# Define the Gaussian Naive Bayes model and train the model on the training data
gnb_classifier = NaiveBayes(labelCol="target", featuresCol="features",smoothing=1.0, modelType="gaussian").fit(trainingData)

# Make predictions on the test data
gnb_predictions = gnb_classifier.transform(testData)

# Evaluate the model's performance
gnb_auc = BinaryClassificationEvaluator(labelCol='target').evaluate(gnb_predictions)
print("Gaussian Naive Bayes Test AUC = %g" % gnb_auc)

# Evaluate the model's performance
gnb_evaluator = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="accuracy")
gnb_accuracy = gnb_evaluator.evaluate(gnb_predictions)
print("Gaussian Naive Bayes Test Error = %g" % (1.0-gnb_accuracy))

"""Precision & Recall"""

# Evaluate the performance of the model
gnb_accuracy = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="target", metricName="accuracy").evaluate(gnb_predictions)
print("Gaussian Naive Bayes Accuracy = %g" % gnb_accuracy)

gnb_precision = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="target", metricName="weightedPrecision").evaluate(gnb_predictions)
print("Gaussian Naive Bayes Precision = %g" % gnb_precision)

gnb_recall = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="target", metricName="weightedRecall").evaluate(gnb_predictions)
print("Gaussian Naive Bayes Recall = %g" % gnb_recall)

gnb_f1_score = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="target", metricName="f1").evaluate(gnb_predictions)
print("Gaussian Naive Bayes F1 Score = %g" % gnb_f1_score)

"""Confusion matrix"""

# Get the confusion matrix as a NumPy array
gnb_confusion_matrix = gnb_predictions.groupBy("target", "prediction").count().orderBy("target", "prediction").toPandas().pivot(index="target", columns="prediction", values="count").fillna(0).values

plt.figure(figsize=(5,5))
sns.heatmap(gnb_confusion_matrix, annot=True, fmt="g", vmin=0, cmap="Blues", cbar=False)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Gaussian Naive Bayes Confusion matrix")
plt.show()

df_results = spark.createDataFrame(
         [("Random Forest",rf_auc,rf_precision,rf_recall,rf_f1_score),
          ("Logistic Regression",lr_auc,lr_precision,lr_recall,lr_f1_score),     
          ("Gaussian Naive Bayes",gnb_auc,gnb_precision,gnb_recall,gnb_f1_score)],
          
    ["ML model","AUC","Precision","Recall","F1-score"] 
)

df_results.toPandas()

"""We chose Random Forest	because it has the highest AUC and the highest recall value."""

